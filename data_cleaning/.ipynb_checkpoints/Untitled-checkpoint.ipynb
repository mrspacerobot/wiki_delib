{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697c414c-af86-43e3-a6ae-133df6a9d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mwparserfromhell\n",
    "def find_matching_closed_rfc_tags(text):\n",
    "    \"\"\"\n",
    "    Find the positions of the {{closed rfc top}} and {{closed rfc bottom}} tags in a text and return the position of the\n",
    "    first matching pair of tags.\n",
    "    :param text: The text to search for tags.\n",
    "    :return: A tuple of integers representing the positions of the top and bottom tags for the first matching pair, or None\n",
    "    if no matching pair is found.\n",
    "    \"\"\"\n",
    "    top_tag = '{{closed rfc top'\n",
    "    bottom_tag = '{{closed rfc bottom}}'\n",
    "    top_tag_pos = None\n",
    "    bottom_tag_pos = None\n",
    "    open_tags = []\n",
    "    for x, comment in enumerate(text):\n",
    "        lower = comment['text'].lower()\n",
    "        for i, char in enumerate(lower):\n",
    "                    if char == '{' and lower[i:i+len(top_tag)] == top_tag:\n",
    "                        open_tags.append(x)\n",
    "                    elif char == '{' and lower[i:i+len(bottom_tag)] == bottom_tag:\n",
    "                        if open_tags:\n",
    "                            top_tag_pos = open_tags.pop()\n",
    "                            bottom_tag_pos = x\n",
    "                            if not open_tags:\n",
    "                                return (top_tag_pos, bottom_tag_pos)                     \n",
    "    if len(open_tags) == 1:\n",
    "        return open_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38116672-25fb-495d-af60-83216526e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_parsed = \"../json_files/grawitas_output/wikipedia_parsed.json\"\n",
    "with open(wikipedia_parsed) as f:\n",
    "        wikipedia_list_of_dicts = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e6057f-c47a-4949-9eb1-de8d536db8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(wikipedia_list_of_dicts, \"page_text\", [\"page_title\",\"page_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5912f600-e544-4c93-888d-b9092293823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in wikipedia_list_of_dicts:\n",
    "    if page['page_id'] == 61466044:\n",
    "        print(True)\n",
    "    if page['page_text'] == None:\n",
    "        wikipedia_list_of_dicts.remove(page)\n",
    "    else:\n",
    "        # convert json object to tuple\n",
    "        page_id = (page['page_id'],str(page['page_text']))\n",
    "        # check if the tuple is already in the set\n",
    "        if page_id in unique_objs:\n",
    "            # remove duplicate object\n",
    "            wikipedia_list_of_dicts.remove(page)\n",
    "        else:\n",
    "            # add the tuple to the set of unique objects\n",
    "            unique_objs.add(page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf6cf3b-d366-4d0f-9449-f2216c47b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicates\n",
    "# create a set to store unique tuples of objects\n",
    "unique_objs = set()\n",
    "\n",
    "# loop through the list of json objects\n",
    "for i, obj in enumerate(wikipedia_list_of_dicts):\n",
    "    if page['page_id'] == 61466044:\n",
    "        print(True)\n",
    "    if obj['page_text'] == None:\n",
    "        wikipedia_list_of_dicts.pop(i)\n",
    "    else:\n",
    "        # convert json object to tuple\n",
    "        page_id = (obj['page_id'],str(obj['page_text']))\n",
    "        # check if the tuple is already in the set\n",
    "        if page_id in unique_objs:\n",
    "            # remove duplicate object\n",
    "            wikipedia_list_of_dicts.pop(i)\n",
    "        else:\n",
    "            # add the tuple to the set of unique objects\n",
    "            unique_objs.add(page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e6134-6357-426c-961c-1dcc52dd0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checking if comments on page are in the closed rfc\n",
    "\"\"\"\n",
    "rf_table = {}\n",
    "loc = None\n",
    "count = 0\n",
    "comment_counter = 0\n",
    "for page in wikipedia_list_of_dicts:\n",
    "    if page['page_id'] == 61466044:\n",
    "        print('yes')\n",
    "    loc = find_matching_closed_rfc_tags(page['page_text'])\n",
    "    if loc == None:\n",
    "        wikipedia_list_of_dicts.remove(page)\n",
    "        continue\n",
    "    elif len(loc) == 1:\n",
    "        page['page_text'] = page['page_text'][loc[0]:]\n",
    "    elif len(loc) == 2:\n",
    "        page['page_text'] = page['page_text'][loc[0]:loc[1]]\n",
    "\n",
    "    rf_table[count] = {\"discussion_title\" : page['page_text'][0]['section'], \"discussion_result_comment_id\" : comment_counter, \"discussion_input_comment\" : comment_counter+1}\n",
    "\n",
    "    dif =  page['page_text'][0]['id'] - comment_counter\n",
    "    for i, comment in enumerate(page['page_text']):\n",
    "        comment['rfc_id'] = int(count)\n",
    "        comment['id'] = comment_counter\n",
    "        comment['project'] = 'wikipedia'\n",
    "\n",
    "        if comment['parent_id'] != 0:\n",
    "            comment['parent_id'] = comment['parent_id'] - dif\n",
    "        comment_counter = comment_counter + 1\n",
    "    count = count + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d463830-5b26-4726-99ec-a875f819f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(wikipedia_list_of_dicts, \"page_text\", [\"page_title\",\"page_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0d4f4-e139-45fc-af32-339ad869c834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95805cd7-0c4e-49ea-9dac-ab8205b49260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c433f-a02b-4800-9319-d639c08f8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df = pd.DataFrame(rf_table)\n",
    "rfc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52cefa-9176-411b-b8f8-c5e123699cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTemplateVisualText(template):\n",
    "    \"\"\"\n",
    "    Gets the HTML for a template that is displayed and returns as simple text\n",
    "    \"\"\"\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"expandtemplates\",\n",
    "        \"text\": f'{template}',\n",
    "        \"prop\": \"wikitext\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    if R: \n",
    "        DATA = R.json()\n",
    "        soup = BeautifulSoup(DATA['expandtemplates']['wikitext'], 'html.parser')\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        return str(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a23a94-7eba-48cb-988a-1eaad427fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f4d68-c653-4737-bcf8-fa60848d01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef761716-eaba-44c3-a67e-77455a0ce420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0,len(df1))):\n",
    "    text = df1.at[i,'text']\n",
    "    templates = mwparserfromhell.parse(text).filter_templates()\n",
    "    for template in templates:\n",
    "        string = template.__str__()\n",
    "        if string not in template_dic:\n",
    "            template_html = getTemplateVisualText(string)\n",
    "            template_dic[string] = template_html\n",
    "            text = text.replace(string, template_html)\n",
    "        else:\n",
    "            text = text.replace(string, template_dic[string])\n",
    "    df1.at[i,'text'] = mwparserfromhell.parse(text).strip_code(collapse = True,keep_template_params=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7bb2c-b180-445e-aec8-4370fe852481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865f979-db51-4e92-b310-0a961c2e3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelling = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5ec7a-8dc3-4c86-bcf3-8fbe8eb6d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate URLs based on page IDs\n",
    "def get_wikipedia_url(page_id):\n",
    "    return f'https://en.wikipedia.org/wiki?curid={page_id}'\n",
    "\n",
    "# Apply the function to the page_id column and assign the result to a new column\n",
    "df_labelling['page_url'] = df_labelling['page_id'].apply(get_wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27755cb-df48-4493-9535-a985264b4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strings to add as new columns\n",
    "new_cols = ['narrative', 'question', 'response', 'advocacy', 'public_interest',\n",
    "    'respect', 'explanation', 'causal_reasoning','counterarguments', 'constructive_proposal']\n",
    "\n",
    "# Add the new columns to the DataFrame\n",
    "for col in new_cols:\n",
    "    df_labelling[col] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490176b-ad05-4e47-9c7e-57a4b2fe3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelling = df_labelling.drop([\"date\",\"section\",\"id\",\"page_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc13718-3144-456e-a47f-d0dd1dc0c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the DataFrame as a CSV file\n",
    "gfg_csv_data = df_labelling.to_csv('original_rfc_statements.csv', index = False)\n",
    "print('\\nCSV String:\\n', gfg_csv_data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
