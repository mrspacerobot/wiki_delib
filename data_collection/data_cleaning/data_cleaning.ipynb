{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "697c414c-af86-43e3-a6ae-133df6a9d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_cleaning\n",
    "import json\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import mwparserfromhell as mw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38116672-25fb-495d-af60-83216526e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_parsed = \"../../data/json_files/grawitas_output/wikipedia_parsed.json\"\n",
    "wikidata_parsed = \"../../data/json_files/grawitas_output/wikidata_parsed.json\"\n",
    "meta_parsed = \"../../data/json_files/grawitas_output/meta_parsed.json\"\n",
    "with open(wikipedia_parsed) as f:\n",
    "        wikipedia_list_of_dicts = json.load(f)  \n",
    "\n",
    "with open(wikidata_parsed) as f:\n",
    "        wikidata_list_of_dicts = json.load(f) \n",
    "        \n",
    "with open(meta_parsed) as f:\n",
    "        meta_list_of_dicts = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c150ed13-c51d-4a86-bfd9-b673dfc5e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_comments, wiki_rfc = data_cleaning.get_RFC_Comment_Table(wikipedia_list_of_dicts, wikidata_list_of_dicts, meta_list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae590ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add \n",
    "import sys\n",
    "sys.path.append('/home/dylan/python-mwchatter/')\n",
    "import wikichatter as wc\n",
    "text = \"{{rfc subpage\\n |status  = globalban-no\\n |comment = This RfC is closed and no global ban is implemented. As stated at [[Stewards'_noticeboard/Archives/2022-04#Global_ban_for_Niveles|the stewards' noticeboard]], we believe that the level of participation in this RfC do not reflect a broad and clear consensus, as required by the global ban policy, to enact one.\\n |date    = 20220207190541 <!-- creation date, do **not** change -->\\n}}\"\n",
    "parsed_text = wc.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db747da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sections': [{'subsections': [], 'comments': [{'text_blocks': [\"{{rfc subpage\\n |status  = globalban-no\\n |comment = This RfC is closed and no global ban is implemented. As stated at [[Stewards'_noticeboard/Archives/2022-04#Global_ban_for_Niveles|the stewards' noticeboard]], we believe that the level of participation in this RfC do not reflect a broad and clear consensus, as required by the global ban policy, to enact one.\\n |date    = 20220207190541 <!-- creation date, do **not** change -->\\n}}\"], 'comments': [], 'cosigners': []}]}]}\n"
     ]
    }
   ],
   "source": [
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ddd61c9-b4bd-40fb-986c-097150c6bfc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wiki_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m comment_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(\u001b[43mwiki_comments\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_title\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wiki_comments' is not defined"
     ]
    }
   ],
   "source": [
    "comment_df = pd.json_normalize(wiki_comments, \"page_text\", [\"page_title\",\"page_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfa3faae-8e6f-4e25-9779-cbdf2c99505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df = pd.json_normalize(wiki_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6359260e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>rfc_id</th>\n",
       "      <th>discussion_title</th>\n",
       "      <th>discussion_result_comment_id</th>\n",
       "      <th>discussion_input_comment</th>\n",
       "      <th>project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55480246</td>\n",
       "      <td>0</td>\n",
       "      <td>RfC about use of YouTube video as primary sour...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66698580</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Scriptural texts]] ([[WP:RSPSCRIPTURE]])</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3977</td>\n",
       "      <td>2</td>\n",
       "      <td>RfC on article scope</td>\n",
       "      <td>118</td>\n",
       "      <td>119</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4828</td>\n",
       "      <td>3</td>\n",
       "      <td>Request for comment on first sentence of lead</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5032</td>\n",
       "      <td>4</td>\n",
       "      <td>RfC about naming \"Soldier F\" in the Bloody Sun...</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_id  rfc_id                                   discussion_title  \\\n",
       "0  55480246       0  RfC about use of YouTube video as primary sour...   \n",
       "1  66698580       1         [[Scriptural texts]] ([[WP:RSPSCRIPTURE]])   \n",
       "2      3977       2                               RfC on article scope   \n",
       "3      4828       3      Request for comment on first sentence of lead   \n",
       "4      5032       4  RfC about naming \"Soldier F\" in the Bloody Sun...   \n",
       "\n",
       "   discussion_result_comment_id  discussion_input_comment    project  \n",
       "0                             0                         1  wikipedia  \n",
       "1                            27                        28  wikipedia  \n",
       "2                           118                       119  wikipedia  \n",
       "3                           128                       129  wikipedia  \n",
       "4                           172                       173  wikipedia  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "947ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/json_files/rfc_pages/meta.json\") as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d66d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in meta:\n",
    "    text = page['page_text']\n",
    "    wikicode = mw.parse(text)\n",
    "    rfc_templates = wikicode.filter_templates(matches=lambda template: template.name.matches(\"rfc subpage\"))\n",
    "    #comment_value = rfc_templates[0].get(\"comment\").value.strip()\n",
    "    if \"date\" in rfc_templates[0]:\n",
    "        date_value = rfc_templates[0].get(\"date\").value.strip()\n",
    "        rfc_df.loc[rfc_df['page_id'] == page['page_id'], 'closing_date'] = date_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0218868",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/json_files/rfc_pages/wikidata.json\") as f:\n",
    "    wikidata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c5e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for page in wikidata:\n",
    "    text = wikidata[0]['page_text']\n",
    "    wikicode = mw.parse(text)\n",
    "    rfc_templates = wikicode.filter_templates(matches=lambda template: template.name.matches(\"discussion top\"))\n",
    "    #comment_value = rfc_templates[0].get(\"comment\").value.strip()\n",
    "    text = rfc_templates[0].params[0]\n",
    "    date = re.search(r'\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4} \\(UTC\\)', str(text))\n",
    "    # parse the input string into a datetime object\n",
    "    input_datetime = datetime.strptime(date.group(), \"%H:%M, %d %B %Y (%Z)\")\n",
    "\n",
    "    # format the datetime object in the desired output format\n",
    "    output_str = input_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    if output_str:\n",
    "        rfc_df.loc[rfc_df['page_id'] == page['page_id'], 'closing_date'] = date_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac8de440",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/json_files/rfc_pages/wikipedia.json\") as f:\n",
    "    wikipedia = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b664042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for page in wikipedia:\n",
    "    text = wikidata[0]['page_text']\n",
    "    wikicode = mw.parse(text)\n",
    "    rfc_templates = wikicode.filter_templates(matches=lambda template: template.name.matches(\"closed rfc top\"))\n",
    "    #comment_value = rfc_templates[0].get(\"comment\").value.strip()\n",
    "    if rfc_templates:\n",
    "        text = rfc_templates[0].params[0]\n",
    "        date = re.search(r'\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4} \\(UTC\\)', str(text))\n",
    "        # parse the input string into a datetime object\n",
    "        input_datetime = datetime.strptime(date.group(), \"%H:%M, %d %B %Y (%Z)\")\n",
    "\n",
    "        # format the datetime object in the desired output format\n",
    "        output_str = input_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        if output_str:\n",
    "            rfc_df.loc[rfc_df['page_id'] == page['page_id'], 'closing_date'] = date_value\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "408c9e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006-09-30\n"
     ]
    }
   ],
   "source": [
    "print(date_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cc86e-7e73-4216-8309-daf8ca9ed8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.templatesToReadableText(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b7b98-273d-47cf-ae92-6f1043e165ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectLanguage(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        url = re.findall(regex, text)\n",
    "        if url:\n",
    "            return \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260b80d-c3f6-475e-8758-bb27a8ce660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regular expression pattern for matching non-word characters\n",
    "non_word_pattern = re.compile(r'^\\W*$')\n",
    "\n",
    "non_alpha_pattern = re.compile(r'^[^a-zA-Z]+$')\n",
    "\n",
    "# Define regex pattern\n",
    "pattern = \"\\s*15px(?:\\|[a-zA-Z]+=\\s*)?(?:\\|[a-zA-Z]+=)?(?:\\|[a-zA-Z]+\\s*)?\"\n",
    "\n",
    "comment_df['text'] = comment_df['text'].str.replace(pattern, '')\n",
    "\n",
    "# filter dataframe to remove rows that contain only non-alpha characters\n",
    "comment_df = comment_df[~comment_df['text'].str.contains(non_alpha_pattern)]\n",
    "\n",
    "# filter dataframe to remove rows that contain only non-word characters\n",
    "comment_df = comment_df[~comment_df['text'].str.contains(non_word_pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26fb85-1afb-490a-a2ce-aca1424e3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Add a new column to the DataFrame indicating the language of the text\n",
    "comment_df['language'] = comment_df['text'].progress_apply(detectLanguage)\n",
    "\n",
    "comment_df = comment_df[comment_df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "456c5be2-7a97-4d89-af1a-b11c5c109594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV String:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "gfg_csv_data = rfc_df.to_csv('../../data/rfc.csv', index = False)\n",
    "print('\\nCSV String:\\n', gfg_csv_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e771c9-55d4-4d24-b311-4f5d600bedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment_df.loc[60187,'date'] = '2022-03-22T23:36:00Z'\n",
    "gfg_csv_data = comment_df.to_csv('../../data/rfc_comments.csv', index = False)\n",
    "print('\\nCSV String:\\n', gfg_csv_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb016f9-9e39-4002-8351-16243a86382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.read_csv('../../data/rfc_comments.csv')\n",
    "def getcleanDataFrame(df):\n",
    "    #remove IP-Adresses from userArray\n",
    "    df = df[~df['user'].str.contains(re.compile(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'))]\n",
    "    return list(df[\"user\"].unique())\n",
    "\n",
    "user_list = getcleanDataFrame(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4b62c5-e9f4-4f61-bf84-bceb9dbfa45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9348\n"
     ]
    }
   ],
   "source": [
    "print(len(user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf13f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserInfoToJSON(userArray, output):\n",
    "    \"\"\"\n",
    "    Takes list of users outputs list of JSON objects containing wiki projects, user rights, edit count, registration and first comment dates\n",
    "    \"\"\" \n",
    "    with multiprocessing.Pool(processes=8) as pool:\n",
    "        results = list(tqdm.tqdm(pool.imap(worker, userArray)))\n",
    "        with open(output, 'w') as file:\n",
    "            json.dump(list(results), file)\n",
    "    \n",
    "    \n",
    "def worker(user):\n",
    "    try:\n",
    "        userDic = getUserInfoAcrossAllReplicaDatabases(user)\n",
    "        return userDic\n",
    "    except:\n",
    "        print(f\"failed to get userDic, with {user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524995d9-c828-4c78-8c40-bce801869397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [02:28,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to get userDic, with SusunWfailed to get userDic, with Nishidanifailed to get userDic, with Triggerhippie4failed to get userDic, with Zero0000failed to get userDic, with Borsoka\n",
      "\n",
      "failed to get userDic, with Alx-plfailed to get userDic, with Cebr1979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muserinformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getUserInfoToJSON\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgetUserInfoToJSON\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data/json_files/user_info/users.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/delib_wiki/data_collection/data_cleaning/userinformation.py:191\u001b[0m, in \u001b[0;36mgetUserInfoToJSON\u001b[0;34m(userArray, output)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mTakes list of users outputs list of JSON objects containing wiki projects, user rights, edit count, registration and first comment dates\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 191\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserArray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    193\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mlist\u001b[39m(results), file)\n",
      "File \u001b[0;32m/srv/paws/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from userinformation import getUserInfoToJSON\n",
    "getUserInfoToJSON(user_list, \"../../data/json_files/user_info/users.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0dafd10a-80b8-416b-92ab-01120826d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by project type and rfc_id\n",
    "grouped = comment_df.groupby(['project', 'rfc_id'])\n",
    "\n",
    "# Define a function to remove the first 2 comments per different rfc_id\n",
    "def remove_comments(group):\n",
    "    if len(group) > 2:\n",
    "        return group.iloc[2:]\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Apply the function to each group and concatenate the results\n",
    "filtered = grouped.apply(remove_comments).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9bf9b14d-eada-49b8-a981-290eb9c5e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dataframe by class and sample 65 rows from each group\n",
    "df_labelling = filtered.groupby('project').apply(lambda x: x.sample(65)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65021394-5b23-4769-bb27-e7918822905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate URLs based on page IDs\n",
    "def get_wikipedia_url(row):\n",
    "    page_id = row['page_id']\n",
    "    project = row['project']\n",
    "    return f'https://{project}.org/wiki?curid={page_id}'\n",
    "\n",
    "# Apply the function to the page_id column and assign the result to a new column\n",
    "df_labelling['page_url'] = df_labelling.apply(get_wikipedia_url, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f27755cb-df48-4493-9535-a985264b4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of strings to add as new columns\n",
    "new_cols = [ 'disrespect','respect','explanation','causal_reasoning','narrative', 'question', 'response', 'advocacy', 'public_interest','counterarguments', 'constructive_proposal']\n",
    "\n",
    "# Add the new columns to the DataFrame\n",
    "for col in new_cols:\n",
    "    df_labelling[col] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0490176b-ad05-4e47-9c7e-57a4b2fe3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelling = df_labelling.drop([\"date\",\"section\",\"page_id\", \"rfc_id\", \"parent_id\", \"language\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3dc13718-3144-456e-a47f-d0dd1dc0c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSV String:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "# saving the DataFrame as a CSV file\n",
    "gfg_csv_data = df_labelling.to_csv('label_rfc_statements.csv', index = False)\n",
    "print('\\nCSV String:\\n', gfg_csv_data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
