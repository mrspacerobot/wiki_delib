{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b7726d-0eab-42f8-8bcc-522bebe90a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "import json\n",
    "import mwparserfromhell\n",
    "import re\n",
    "from pywikibot import pagegenerators as pg\n",
    "\n",
    "def list_template_usage(site_obj, tmpl_name):\n",
    "    \"\"\"\n",
    "    Takes Site object and template name and returns a generator.\n",
    "\n",
    "    The function expects a Site object (pywikibot.Site()) and\n",
    "    a template name (String). It creates a list of all\n",
    "    pages using that template and returns them as a generator.\n",
    "    The generator will load 50 pages at a time for iteration.\n",
    "    \"\"\"\n",
    "    name = \"{}:{}\".format(site.namespace(10), tmpl_name)\n",
    "    tmpl_page = pywikibot.Page(site, name)\n",
    "    ref_gen = tmpl_page.getReferences(follow_redirects=False)\n",
    "    #filter_gen = pg.NamespaceFilterPageGenerator(ref_gen, namespaces=[121])\n",
    "    generator = site.preloadpages(ref_gen, pageprops=True)\n",
    "    return generator\n",
    "\n",
    "def gather_template_usage(generator, template_name, completedDiscussionParameterValueList=None, header=None):\n",
    "    \"\"\"\n",
    "    Takes a generator and a template name and returns usage list.\n",
    "\n",
    "    The function can also take a header (list of strings) that will be\n",
    "    the headers of the table (Needs to be the same dimension as the table).\n",
    "    The first column needs to be a link to property (It will be made into\n",
    "    a link. In this example the second column is a list of links to Q-items.\n",
    "    \"\"\"\n",
    "    tmpl_usage = []\n",
    "    dic_json_list = []\n",
    "    if header != None:\n",
    "        tmpl_usage.append(header)\n",
    "    page_title_list = []\n",
    "    for page in generator:\n",
    "        page_str = page.get()\n",
    "        tmpl_list = pywikibot.textlib.extract_templates_and_params(page_str)\n",
    "\n",
    "        for tmpl in tmpl_list:\n",
    "            if template_name in tmpl:\n",
    "                wikicode = mwparserfromhell.parse(page_str)\n",
    "                wikicode_sections = wikicode.get_sections()\n",
    "                section_index = -1\n",
    "                for count, section in enumerate(wikicode_sections):\n",
    "                    node = section.filter_templates(matches=\"closed rfc top\")\n",
    "                    if node:\n",
    "                        section_index = count\n",
    "                if section_index >= 0:\n",
    "                    page_title = page.title()\n",
    "                    page_id = page.pageid\n",
    "                    dic = {\"page_id\" : page_id, \"page_title\" : page_title, \"page_text\" : str(wikicode_sections[section_index])}\n",
    "                    dic_json_list.append(dic)\n",
    "                    page_title_list.append(page_title)\n",
    "\n",
    "                if completedDiscussionParameterValueList and tmpl[1]:\n",
    "                    dic = tmpl[1]\n",
    "                    status = list(dic.items())[0][1]\n",
    "                    if status in completedDiscussionParameterValueList:\n",
    "                        page_title = page.title()\n",
    "                        page_id = page.pageid\n",
    "                        dic = {\"page_id\" : page_id, \"page_title\" : page_title, \"page_text\" : page_str}\n",
    "                        dic_json_list.append(dic)\n",
    "                        page_title_list.append(page_title)\n",
    "                    \n",
    "    out_file = open(f'wikipedia.json', \"w\")\n",
    "    json.dump(dic_json_list,out_file,indent = 6)\n",
    "    out_file.close()\n",
    "    return page_title_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4200947-a8b5-48ff-9c7f-54bebf93b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 50 pages from wikipedia:en.\n",
      "Retrieving 37 pages from wikipedia:en.\n"
     ]
    }
   ],
   "source": [
    "site = pywikibot.Site(\"en\", 'wikipedia')\n",
    "\n",
    "wikipedia_template_name = \"closed rfc top\"\n",
    "\n",
    "wikipedia_tmpl_gen = list_template_usage(site, wikipedia_template_name)\n",
    "\n",
    "wikipedia_tmpl_usage = gather_template_usage(wikipedia_tmpl_gen, wikipedia_template_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
